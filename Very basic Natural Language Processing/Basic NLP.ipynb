{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Use basic NLP to recognise food orders. A food order includes food, its ingredients and quantity desired. This exercise requires food orders to be in Spanish. Given an input food order, must return the food (comida), ingredients (ingredientes), and quantity (cantidad). There is no need to construct an intention classifier as the intention of all phrases is 'order food'. There is also no need to normalise the output. For example, it's not necessary to convert 'tres' to '3', nor 'pizzas' to 'pizza'.\n",
    "\n",
    "An example input food order and the output would be:\n",
    "\n",
    "    'Quiero 3 bocadillos de anchoas y 2 pizzas' -> \n",
    "    \n",
    "    [\n",
    "        {comida:'bocadillo', ingrediente:'anchoas', cantidad:'3'},\n",
    "        {comida:'pizza', ingrediente:'null', cantidad:'2'}\n",
    "    ]\n",
    "     \n",
    "The output is array of 2 dictionaries of 3 keys each (food, ingredient and quantity). When a quantity is not detected, the default quantity is 1.\n",
    "\n",
    "Chapter 7 of the NLTK book Natural Language Processing with Python by Bird, Klein, and Loper is very instructive. Particularly section 3.3 (Training classifier-based chunkers)\n",
    "\n",
    "Will need to use 4 different approaches; using `RegexpParser`, a unigram tagger, a bigram tagger, and `NaiveBayesClassifier` from the NLTK library.\n",
    "\n",
    "Training phrases must be in Spanish, and POS tagger are not very good Spanish, so low precision can be expected.\n",
    "\n",
    "Need to constructan NLP chain with NLTK with the following elements:\n",
    "* Segmentation of phrases\n",
    "* Tokenisation\n",
    "* POS tagger\n",
    "\n",
    "The POS tags will be used for the RegexParser, UnigramParser, BigramParser, and the NaiveBayesClassifier\n",
    "\n",
    "For the RegexParser, there is no need to create a training corpus. Specific grammar will need to be generated, and pass that to the `RegexpParser` function of NLTK.\n",
    "\n",
    "For the other methods, a training corpus needs to be created in the IOB format:\n",
    "\n",
    "    yo PRP I                        Personal Pronoun\n",
    "    quería VBD I                    Verb Past Tense\n",
    "    una DT I-Cantidad               Determiner\n",
    "    hamburguesa NN I-Comida         Noun, singular or mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish POS Tagger, Corpus Creation, Grammar Generation\n",
    "\n",
    "* We will begin by importing the libraries that we will use for this entire notebook\n",
    "* Then we will import a Spanish corpus that contains 188,650 words that have been syntactically annotated\n",
    "* We will then split the corpus into training and test\n",
    "* Next, we will train our Spanish POS tagger useing the training corpus and evaluate it's accuracy with the test corpus\n",
    "* Then we will create and tag our own corpus for the purposes of this exercise\n",
    "* Finally, we will generate the grammar required to chunk our corpus into the desired chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp # We will use this corpus to train our spanish pos tagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger # This will be our spanish pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cess_sents = cess_esp.tagged_sents() # Get tagged sentences from corpus\n",
    "\n",
    "\n",
    "# Split tagged sentences into 90% train, 10% test\n",
    "\n",
    "training = [] \n",
    "test = []\n",
    "\n",
    "for i in range(len(cess_sents)):\n",
    "    if i % 10:\n",
    "        training.append(cess_sents[i])\n",
    "    else:\n",
    "        test.append(cess_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train spanish pos tagger\n",
    "\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger has an accuracy of 89.89 %\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy of our spanish pos tagger\n",
    "\n",
    "print('Tagger has an accuracy of' , round(hmm_tagger.evaluate(test)*100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a corpus of food orders\n",
    "sents = ['Yo quiero 3 hamburguesas', 'Yo quería 3 bocadillos de jamón', 'Yo quiero una pizza',\n",
    "         'Queremos un bocadillo de pavo', 'Yo quiero 4 croquetas de jamón', 'Quiero un brownie',\n",
    "         'Quiero cuatro galletas', 'Quiero una tarta de queso', 'Queiro 3 tacos al pastor', 'Quiero una Coca Cola',\n",
    "         'Quiero 6 alitas de pollo']\n",
    "\n",
    "sents_tagged = [] # Empty list to store tagged phrases\n",
    "\n",
    "# Iterate through each phrase to obtain tags\n",
    "for sent in sents:\n",
    "    tokens = nltk.word_tokenize(sent) # Tokenize phrase\n",
    "    tokens_tagged = hmm_tagger.tag(tokens) # Tag tokens\n",
    "    sents_tagged.append(tokens_tagged) # Append tokens to sents_tagged list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate grammar to chunk 'Cantidad', 'Ingrediente', and 'Comida' for each order \n",
    "grammar = r\"\"\"\n",
    "        Cantidad: {<Z|di.*|c.*|dn.*>}\n",
    "        Ingrediente: {<sp.*><da.*|ncm.*>}\n",
    "        Comida: {<nc.*|sn.e-SUJ>*<aq.*>?}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quantity is detected when a token is tagged with `Z` or any tags beginning with `di`, `c`, or `dn`.\n",
    "<br></br>\n",
    "* Ingrediente is detected when a token is tagged with any tag beginning with `sp` followed by any tag beginning with `da` or `ncm`.\n",
    "    * the `sp` tag refers to 'de' as in 'tarta de chocolate'  \n",
    "<br></br>\n",
    "* Comida is detected when a token is tagged with any number of tags beginning with `nc` or `sn.e-SUJ` followed by any tag beginning with `aq` should such a tag exist after the previous.\n",
    "    * The `aq` tag is optional is detecting 'Comida' and is not necessary to generate a 'Comida' chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Parser\n",
    "\n",
    "* First we will create our parser using nltk's `RegexpParser` and the grammar written above\n",
    "* Then we will parse our tagged corpus using our regex parser\n",
    "* Then we will go through each parsed phrases tree to extract the desired inforation, storing each order in a dictionary, and storing all order dictionaries in a list\n",
    "* Finally, we will print each order to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [('Yo', 'pp1csn00'), ('quiero', 'vmip1s0'), Tree('Cantidad', [('3', 'cs')]), Tree('Comida', [('hamburguesas', 'sn.e-SUJ')])]), Tree('S', [('Yo', 'pp1csn00'), ('quería', 'vmii3s0'), Tree('Cantidad', [('3', 'Z')]), Tree('Comida', [('bocadillos', 'ncmp000')]), Tree('Ingrediente', [('de', 'sps00'), ('jamón', 'ncms000')])]), Tree('S', [('Yo', 'pp1csn00'), ('quiero', 'vmip1s0'), Tree('Cantidad', [('una', 'di0fs0')]), Tree('Comida', [('pizza', 'ncfs000')])]), Tree('S', [('Queremos', 'sps00'), Tree('Cantidad', [('un', 'di0ms0')]), Tree('Comida', [('bocadillo', 'ncms000')]), Tree('Ingrediente', [('de', 'sps00'), ('pavo', 'da0fs0')])]), Tree('S', [('Yo', 'pp1csn00'), ('quiero', 'vmip1s0'), Tree('Cantidad', [('4', 'di0fp0')]), Tree('Comida', [('croquetas', 'ncfp000')]), Tree('Ingrediente', [('de', 'sps00'), ('jamón', 'ncms000')])]), Tree('S', [('Quiero', 'sps00'), Tree('Cantidad', [('un', 'di0ms0')]), Tree('Comida', [('brownie', 'ncms000')])]), Tree('S', [('Quiero', 'da0mp0'), Tree('Cantidad', [('cuatro', 'dn0cp0')]), Tree('Comida', [('galletas', 'ncmp000')])]), Tree('S', [('Quiero', 'sps00'), Tree('Cantidad', [('una', 'di0fs0')]), Tree('Comida', [('tarta', 'ncfs000')]), Tree('Ingrediente', [('de', 'sps00'), ('queso', 'da0fs0')])]), Tree('S', [('Queiro', 'da0mp0'), Tree('Cantidad', [('3', 'Z')]), Tree('Comida', [('tacos', 'ncmp000')]), Tree('Ingrediente', [('al', 'spcms'), ('pastor', 'ncms000')])]), Tree('S', [('Quiero', 'sps00'), Tree('Cantidad', [('una', 'di0fs0')]), Tree('Comida', [('Coca', 'ncfs000'), ('Cola', 'aq0fs0')])]), Tree('S', [('Quiero', 'da0mp0'), Tree('Cantidad', [('6', 'Z')]), Tree('Comida', [('alitas', 'ncmp000')]), Tree('Ingrediente', [('de', 'sps00'), ('pollo', 'ncms000')])])]\n"
     ]
    }
   ],
   "source": [
    "parser = nltk.RegexpParser(grammar) # Create chunk parser\n",
    "\n",
    "sents_parsed = [] # Emtpy list to store parsed phrases\n",
    "\n",
    "# Parse phrases and store in sents_parsed list\n",
    "\n",
    "for sent in sents_tagged:\n",
    "    results = parser.parse(sent)\n",
    "    sents_parsed.append(results)\n",
    "\n",
    "print(sents_parsed) # Uncomment to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = [] # Empty list to store orders\n",
    "\n",
    "# Iterate over parsed phrases\n",
    "\n",
    "for sent in sents_parsed:\n",
    "    order = {} # Emtpy dictionary to store an order\n",
    "\n",
    "    for n in sent: # Access each node of the tree\n",
    "        if isinstance(n, nltk.tree.Tree): # Returns True as the nodes is a subclass of a tree. This step is necessary and doesn't work otherwise\n",
    "            if n.label() == 'Cantidad': # Check if a chunk is labelled 'Cantidad'\n",
    "                if n.leaves()[0][0] in ['una', 'un']: # Convert 'una' and 'un' to '1'\n",
    "                    order['Cantidad'] = '1' # Add a key value pair to the order dictionary\n",
    "                else:\n",
    "                    order['Cantidad'] = n.leaves()[0][0] # Add the quantity number detecting in the chunking to the 'Cantidad' key of the order dictionary\n",
    "            elif n.label() == 'Comida': # Checks if a chunk is labelled 'Comida'\n",
    "                comida = '' # Empty string for formatting the output\n",
    "                # Format output of 'Comida' to be more easily read if chunk has more than one element\n",
    "                for i in range(len(n.leaves())):\n",
    "                    comida += n.leaves()[i][0] + ' '\n",
    "                order['Comida'] = comida.strip()\n",
    "            if n.label() == 'Ingrediente': # Check if a chunk is labelled 'Ingrediente'\n",
    "                order['Ingrediente'] = n.leaves()[1][0] # Add ingredient to order dictionary\n",
    "                \n",
    "    orders.append(order) # append order dictionary to orders list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 1 : {'Cantidad': '3', 'Comida': 'hamburguesas'}\n",
      "Order 2 : {'Cantidad': '3', 'Comida': 'bocadillos', 'Ingrediente': 'jamón'}\n",
      "Order 3 : {'Cantidad': '1', 'Comida': 'pizza'}\n",
      "Order 4 : {'Cantidad': '1', 'Comida': 'bocadillo', 'Ingrediente': 'pavo'}\n",
      "Order 5 : {'Cantidad': '4', 'Comida': 'croquetas', 'Ingrediente': 'jamón'}\n",
      "Order 6 : {'Cantidad': '1', 'Comida': 'brownie'}\n",
      "Order 7 : {'Cantidad': 'cuatro', 'Comida': 'galletas'}\n",
      "Order 8 : {'Cantidad': '1', 'Comida': 'tarta', 'Ingrediente': 'queso'}\n",
      "Order 9 : {'Cantidad': '3', 'Comida': 'tacos', 'Ingrediente': 'pastor'}\n",
      "Order 10 : {'Cantidad': '1', 'Comida': 'Coca Cola'}\n",
      "Order 11 : {'Cantidad': '6', 'Comida': 'alitas', 'Ingrediente': 'pollo'}\n"
     ]
    }
   ],
   "source": [
    "# Print each order\n",
    "for i in range(len(orders)):\n",
    "    print('Order', i+1, ':', orders[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the cell above we can see the final output using the `RegexpParser`. For the given format of ordering a food item, we can accurately obtain what food is being ordered and in which quantity along with the ingredients.\n",
    "\n",
    "#### Issues encountered and my solutions\n",
    "* Generation of grammar to chunk the desired tokens. The complexity of Spanish POS tags meant having to allow for tags having certain beginnings and not use the entire tag. For example, the tag for 'bocadillos' is 'ncmp000', however, the tag for 'pizza' is 'ncfs000'. These two tags have the first two characters in common so I decided to chunk of those characters.\n",
    "* To chunk ingredients, it made sense to create grammar that found 'de' followed by another noun. This meant having to place 'ingrediente' before 'comida' when writing the grammar. If 'comida' was written first, the actual ingredients would be chunked as food\n",
    "* When tyring to extract the tokens referring to quantity, food, and ingredient. I initally has some trouble accessing the tree for each parsed phrase. Including `isinstance` with `nltk.tree.Tree` data type as the second arguement solved this inital issue. Secondly, I had to 'find' the token in each node. for the quantity leaf, this was relatively simple, only having to convert 'una' and 'un' to '1', otherwise just giving the token as the value to the 'Cantidad' key fo the order dictionary. Slightly more complicated was food. This was due to the inclusion of an order of Coca Cola. In this case, the 'Comida' chunk had 2 elements, both of which I wanted to add to the dictionary under the 'Comida' key. In order to solve this, I created an empty string then added the token with a space, finally adding the filled string, removing the extra whitespace at the end, to the dictionary. Finally, the 'Ingrediente' chunk also had 2 elements 'de' and the 'ingrediente'. Fortunately I only wanted the second element, so accessing its' index is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Chunker and Parser\n",
    "* First we will create our training phrases from the corpus created earlier. This means adding the IOB tags to the tagged phrases\n",
    "* Next we will create a test phrase and tag it with the Spanish POS tagger made earlier\n",
    "* Then we will create the `UnigramChunker` class object adapted from Chapter 7 of the Natural Language Processing with Python book by Bird, Klein, and Loper\n",
    "* We will then train the `UnigramChunker` with our training corpus and parse our test phrase\n",
    "* We will then print the order to the screen as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training corpus in the IOB format\n",
    "\n",
    "train_sents = [] # Empty list to store training phrases\n",
    "\n",
    "for sent in sents_parsed: # Iterate through parsed corpus\n",
    "    IOB = nltk.chunk.tree2conlltags(sent) # Apply IOB tags to each phrase\n",
    "    train_sents.append(IOB) # Append phrases in IOB format to train_sents list\n",
    "    \n",
    "# print(train_sents) # Uncomment to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = 'Yo quiero 4 croquetas de jamón' # Create test phrase/order\n",
    "test_tokens = nltk.word_tokenize(test_sent) # Tokenize\n",
    "test_tagged = hmm_tagger.tag(test_tokens) # Tag tokens with Spanish POS tagger\n",
    "\n",
    "# print(test_tagged) # Uncomment to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Unigram Chunker\n",
    "\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        # Train UnigramTagger with training corpus\n",
    "        train_data = [[(t,c) for w,t,c in sent] for sent in train_sents] # Extract tags and chunk tags for each phrase/order\n",
    "\n",
    "        self.tagger = nltk.UnigramTagger(train_data) # Train tagger and create tagger object for parsing new phrases\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        # Parse POS tagged phrase/order\n",
    "        pos_tags = [pos for word,pos in test_tagged] # Extract POS tag\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags) # Apply IOB tag using trained tagger\n",
    "        chunktags = [chunktag for tag,chunktag in tagged_pos_tags] # Extract chunk tag \n",
    "        conlltags = [(word,pos,chunktag) for ((word,pos),chunktag) in zip(test_tagged, chunktags)] # Put token, POS tag and chunk tag together in IOB format \n",
    "        return nltk.conlltags2tree(conlltags) # return IOB format tree for each phrase/order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Yo/pp1csn00\n",
      "  quiero/vmip1s0\n",
      "  (Cantidad 4/di0fp0)\n",
      "  (Comida croquetas/ncfp000)\n",
      "  (Ingrediente de/sps00 jamón/ncms000))\n"
     ]
    }
   ],
   "source": [
    "unigram_chunker = UnigramChunker(train_sents) # Create and train chunker object\n",
    "\n",
    "test_tree = unigram_chunker.parse(test_tagged) # Parse POS tagged test phrase/order\n",
    "\n",
    "print(test_tree) # Uncomment to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cantidad': '4', 'Comida': 'croquetas', 'Ingrediente': 'jamón'}\n"
     ]
    }
   ],
   "source": [
    "order = {} # Empty dictionary to store order\n",
    "\n",
    "for n in test_tree: # Iterate through nodes of tree\n",
    "    if isinstance(n, nltk.tree.Tree): # As above, returns True\n",
    "        if n.label() == 'Cantidad': # Check if a chunk is labelled 'Cantidad'\n",
    "            if n.leaves()[0][0] in ['una', 'un']: # Convert 'una' and 'un' to '1'\n",
    "                order['Cantidad'] = '1' # Add a key value pair to the order dictionary\n",
    "            else:\n",
    "                order['Cantidad'] = n.leaves()[0][0] # Add the quantity number detecting in the chunking to the 'Cantidad' key of the order dictionary\n",
    "        elif n.label() == 'Comida': # Check if a chunk is labelled 'Comida'\n",
    "            order['Comida'] = n.leaves()[0][0] # Add food to 'Comida' key of order dictionary\n",
    "        elif n.label() == 'Ingrediente': # Check if a chunk is labelled 'Ingrediente'\n",
    "            order['Ingrediente'] = n.leaves()[1][0] # Add ingredient to order dictionary\n",
    "                \n",
    "\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The final output using a `UnigramChunker` and `UnigramTagger` is as expected, detecteing accurately the desired chunks.\n",
    "\n",
    "#### Issues encountered and my solutions\n",
    "* The only real issue encountered in this section of the exercise was adapting the UnigramChunker form the Natural Language Processing with Python book. Initially, just copying the code written in that book did not work with the format of the trianing corpus. This was due to the training corpus already being in the IOB format. Therefore removing `nltk.chunk.tree2conlltags` from the list comprehension for genereating the training data within the `UnigramChunker` class solved this issue.\n",
    "* Aside from this there were no other issues of note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Chunker and Parser\n",
    "* Next we will create a Bigram chunker and parser in the same way we created the Unigram chunker and parser\n",
    "* We already have the trainging corpus and test phrase so all that is needed is to adapt the `UnigramChunker` to used the `BigramTagger` instead of the `UnigramTagger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bigram Chunker\n",
    "\n",
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        # Train BigramTagger with training corpus\n",
    "        train_data = [[(t,c) for w,t,c in sent] for sent in train_sents] # Extract tags and chunk tags for each phrase/order\n",
    "\n",
    "        self.tagger = nltk.BigramTagger(train_data) # Train tagger and create tagger object for parsing new phrases\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        # Parse POS tagged phrase/order\n",
    "        pos_tags = [pos for word,pos in test_tagged] # Extract POS tag\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags) # Apply IOB tag using trained tagger\n",
    "        chunktags = [chunktag for tag,chunktag in tagged_pos_tags] # Extract chunk tag \n",
    "        conlltags = [(word,pos,chunktag) for ((word,pos),chunktag) in zip(test_tagged, chunktags)] # Put token, POS tag and chunk tag together in IOB format \n",
    "        return nltk.conlltags2tree(conlltags) # return IOB format tree for each phrase/order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Yo/pp1csn00\n",
      "  quiero/vmip1s0\n",
      "  (Cantidad 4/di0fp0)\n",
      "  (Comida croquetas/ncfp000)\n",
      "  (Ingrediente de/sps00 jamón/ncms000))\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(train_sents) # Create and train chunker object\n",
    "\n",
    "test_tree = bigram_chunker.parse(test_tagged) # Parse POS tagged test phrase/order\n",
    "\n",
    "print(test_tree) # Uncomment to print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cantidad': '4', 'Comida': 'croquetas', 'Ingrediente': 'jamón'}\n"
     ]
    }
   ],
   "source": [
    "order = {} # Empty dictionary to store order\n",
    "\n",
    "for n in test_tree: # Iterate through nodes of tree\n",
    "    if isinstance(n, nltk.tree.Tree): # As above, returns True\n",
    "        if n.label() == 'Cantidad': # Check if a chunk is labelled 'Cantidad'\n",
    "            if n.leaves()[0][0] in ['una', 'un']: # Convert 'una' and 'un' to '1'\n",
    "                order['Cantidad'] = '1' # Add a key value pair to the order dictionary\n",
    "            else:\n",
    "                order['Cantidad'] = n.leaves()[0][0] # Add the quantity number detecting in the chunking to the 'Cantidad' key of the order dictionary\n",
    "        elif n.label() == 'Comida': # Check if a chunk is labelled 'Comida'\n",
    "            order['Comida'] = n.leaves()[0][0] # Add food to 'Comida' key of order dictionary\n",
    "        elif n.label() == 'Ingrediente': # Check if a chunk is labelled 'Ingrediente'\n",
    "            order['Ingrediente'] = n.leaves()[1][0] # Add ingredient to order dictionary\n",
    "                \n",
    "\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is completely expected as the `BigramTagger` adds little complexity and fidelity due to the simple nature of the training corpus and test phrase.\n",
    "\n",
    "#### There were no issue encoutered adapting the `UnigramChunker` to a `BigramChunker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-Bayes Classifier for Chunking and Parsing\n",
    "\n",
    "* First we create a function adapted from Chapter 7 of Natural Language Processing with Python to extract the feature of ou training phrases. In this case we want to extract the POS tags and the IOB chunk tags\n",
    "* Next we create a `NaiveBayesChunkTagger` class, also adapted from Chapter 7, that uses the trianing corpus for training a `NaiveBayesClassifier`\n",
    "* Then we create a `NaiveBayesChunker` class, again adapted form Chapter 7, that takes the `NaiveBayesChunkTagger` and is capable of parsing a new phrase to apply IOB chunk tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_features(sentence, i, history):\n",
    "     pos, chunktag = sentence[i]\n",
    "     if i == 0:\n",
    "         prevpos, prevchunktag = \"<START>\", \"<START>\"\n",
    "     else:\n",
    "         prevpos, prevchunktag = sentence[i-1]\n",
    "     if i == len(sentence)-1:\n",
    "         nextpos, nextchunktag = \"<END>\", \"<END>\"\n",
    "     else:\n",
    "         nextpos, nextchunktag = sentence[i+1]\n",
    "     return {\"chunktag\": chunktag,\n",
    "             \"pos\": pos,\n",
    "             \"prevchunktag\": prevchunktag,\n",
    "             \"nextchunktag\": nextchunktag,\n",
    "             \"prevchunktag+chunktag\": \"%s+%s\" % (prevchunktag, chunktag),\n",
    "             \"chunktag+nextchunktag\": \"%s+%s\" % (chunktag, nextchunktag)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesChunkTagger(nltk.TaggerI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for sent in train_sents:\n",
    "            tags_chunks = []\n",
    "            for w,t,c in sent: \n",
    "                tag_chunk = (t,c)\n",
    "                tags_chunks.append(tag_chunk) # Create tags_chunks\n",
    "\n",
    "            history = []\n",
    "            for i, (t,c) in enumerate(tags_chunks): # Enumerate to access index of tags_chunks\n",
    "                    featureset = chunk_features(tags_chunks, i, history) # Extract features using chunk_features defined above\n",
    "                    train_set.append((featureset, c)) # Append features and chunk tag to train_set list\n",
    "                    history.append(c) # Append chunk tag to history\n",
    "\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set) # Train classifier using train_set\n",
    "        \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence): # Access index of input sentence\n",
    "            featureset = chunk_features(sentence, i, history) # Extract features (i.e. POS tags)\n",
    "            tag = self.classifier.classify(featureset) # Tag POS tags with chunk tag\n",
    "            history.append(tag) # Append chunk tag to history\n",
    "        return zip(sentence, history) # Zip input sentence and history for IOB format\n",
    "    \n",
    "class NaiveBayesChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "\n",
    "        tagged_sents = [[(w,t,c) for w,t,c in sent] for sent in train_sents]\n",
    "\n",
    "        self.tagger = NaiveBayesChunkTagger(tagged_sents) # Pass training corpus to tagger\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence) # Tags a new phrase with chunk tags\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents] # Recapitulate sentence with chunk tags for tree creation\n",
    "        return nltk.conlltags2tree(conlltags) # Return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_chunker = NaiveBayesChunker(train_sents) # Define chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Comida\n",
      "    Yo/pp1csn00\n",
      "    quiero/vmip1s0\n",
      "    4/di0fp0\n",
      "    croquetas/ncfp000\n",
      "    de/sps00\n",
      "    jamón/ncms000))\n"
     ]
    }
   ],
   "source": [
    "chunked = NB_chunker.parse(test_tagged) # Parse new phrase\n",
    "print(chunked) # Print tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `NaiveBayesChunker` is not capable of accurately applying IOB chunk tags to the test phrase/order. This appears to be because the `NaiveBayesChunkTagger` is tagging all pos tags as 'inside of chunk', 'I', tags. Therefore when converting back to a tree, the entire phrase is part of a 'Comida' chunk. Preusmably this is due to the small training corpus and that a much larger training corpus, over 100 phrases, might solve this issue. Having said that, I don't understand why all POS tags have been tagged with 'I-Comida' chunk tags as this chunk tag appears very infrequently, even in this small training corpus.\n",
    "<br></br><br></br>\n",
    "Below we can see that each element (pos tag) is assigned the 'I-Comida' IOB chunk tag for the entire training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify NaiveBayesChunker to return the chunk tags rather than tree\n",
    "class NaiveBayesChunker_mod(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "\n",
    "        tagged_sents = [[(w,t,c) for w,t,c in sent] for sent in train_sents]\n",
    "\n",
    "        self.tagger = NaiveBayesChunkTagger(tagged_sents) # Pass training corpus to tagger\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence) # Tags a new phrase with chunk tags\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents] # Recapitulate sentence with chunk tags for tree creation\n",
    "#         return nltk.conlltags2tree(conlltags) # Commented out to show chunk tags\n",
    "        return conlltags # Allows to view chunk tags rather than tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Yo', 'pp1csn00', 'I-Comida'), ('quiero', 'vmip1s0', 'I-Comida'), ('3', 'cs', 'I-Comida'), ('hamburguesas', 'sn.e-SUJ', 'I-Comida')]\n",
      "[('Yo', 'pp1csn00', 'I-Comida'), ('quería', 'vmii3s0', 'I-Comida'), ('3', 'Z', 'I-Comida'), ('bocadillos', 'ncmp000', 'I-Comida'), ('de', 'sps00', 'I-Comida'), ('jamón', 'ncms000', 'I-Comida')]\n",
      "[('Yo', 'pp1csn00', 'I-Comida'), ('quiero', 'vmip1s0', 'I-Comida'), ('una', 'di0fs0', 'I-Comida'), ('pizza', 'ncfs000', 'I-Comida')]\n",
      "[('Queremos', 'sps00', 'I-Comida'), ('un', 'di0ms0', 'I-Comida'), ('bocadillo', 'ncms000', 'I-Comida'), ('de', 'sps00', 'I-Comida'), ('pavo', 'da0fs0', 'I-Comida')]\n",
      "[('Yo', 'pp1csn00', 'I-Comida'), ('quiero', 'vmip1s0', 'I-Comida'), ('4', 'di0fp0', 'I-Comida'), ('croquetas', 'ncfp000', 'I-Comida'), ('de', 'sps00', 'I-Comida'), ('jamón', 'ncms000', 'I-Comida')]\n",
      "[('Quiero', 'sps00', 'I-Comida'), ('un', 'di0ms0', 'I-Comida'), ('brownie', 'ncms000', 'I-Comida')]\n",
      "[('Quiero', 'da0mp0', 'I-Comida'), ('cuatro', 'dn0cp0', 'I-Comida'), ('galletas', 'ncmp000', 'I-Comida')]\n",
      "[('Quiero', 'sps00', 'I-Comida'), ('una', 'di0fs0', 'I-Comida'), ('tarta', 'ncfs000', 'I-Comida'), ('de', 'sps00', 'I-Comida'), ('queso', 'da0fs0', 'I-Comida')]\n",
      "[('Queiro', 'da0mp0', 'I-Comida'), ('3', 'Z', 'I-Comida'), ('tacos', 'ncmp000', 'I-Comida'), ('al', 'spcms', 'I-Comida'), ('pastor', 'ncms000', 'I-Comida')]\n",
      "[('Quiero', 'sps00', 'I-Comida'), ('una', 'di0fs0', 'I-Comida'), ('Coca', 'ncfs000', 'I-Comida'), ('Cola', 'aq0fs0', 'I-Comida')]\n",
      "[('Quiero', 'da0mp0', 'I-Comida'), ('6', 'Z', 'I-Comida'), ('alitas', 'ncmp000', 'I-Comida'), ('de', 'sps00', 'I-Comida'), ('pollo', 'ncms000', 'I-Comida')]\n"
     ]
    }
   ],
   "source": [
    "NB_chunker_mod = NaiveBayesChunker_mod(train_sents) # Train modified chunker\n",
    "\n",
    "# Iterate through training corpus\n",
    "parsed_sents = []\n",
    "for sent in sents_tagged:\n",
    "    chunked = NB_chunker_mod.parse(sent) # Parse phrase with modified chunker\n",
    "    parsed_sents.append(chunked)\n",
    "    \n",
    "for parsed_sent in parsed_sents:\n",
    "    print(parsed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues encountered and my solutions\n",
    "* Issue 1 was having to adapt the `ConsecutiveNPChunkTagger` from HCpater 7 of Natural Language Processing with Python. After breaking is down and understanding what each part was doing, it became clear that the 'untagged_sent' variable was unncessary in this exercise. Also, in this exercise we were attempting to tag POS tags with chunk tags and so the training of the classifier had to be done not using (word, pos) but (pos, chunktag). This was easy enough to fix.\n",
    "* Addtionally, the `chunk_features` function had to be adapted slightly, althoguh this only amounted to removing `tags_since_dt` from the `npchunk_features` function from Chapter 7.\n",
    "* The main issue however, is that described above in that all POS tags appear to be tagged with the 'I-Comida' chunk tag. As said above, perhaps a much larger training corpus might solve this issue. If this is the case, then extracting the desired tokens from the chunked phrases would be as simple as has been done above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
